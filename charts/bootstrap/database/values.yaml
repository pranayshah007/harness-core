# Default values for databases.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  database:
    minio:
      installed: true
    mongodb:
      installed: true
    postgres:
      installed: true
    timescaledb:
      installed: true
    redis:
      installed: true
    clickhouse:
      enabled: false
minio:
  fullnameOverride: "minio"
  mode: standalone
  defaultBuckets: "logs"
  image:
    tag: 2023.7.18-debian-11-r2
  persistence:
    size: 200Gi
  auth:
    existingSecret: "minio"
# -- configurations for mongodb
mongodb:
  architecture: replicaset
  fullnameOverride: "mongodb-replicaset-chart"
  image:
    registry: docker.io
    repository: harness/mongo
    tag: 4.4.22
  replicaCount: 3
  arbiter:
    enabled: true
  service:
    nameOverride: "mongodb-replicaset-chart"
  auth:
    rootUser: "admin"
    existingSecret: "mongodb-replicaset-chart"
  podLabels: { app: mongodb-replicaset }
  resources:
    limits:
      cpu: 4
      memory: 8192Mi
    requests:
      cpu: 4
      memory: 8192Mi
  persistence:
    size: 200Gi
postgresql:
  image:
    registry: docker.io
    repository: bitnami/postgresql
    tag: 14.8.0-debian-11-r17
    digest: ""
  fullnameOverride: "postgres"
  auth:
    # Create the additional ET Database during deployment.
    # Additional init can be added using a configmap
    # primary.initdb.scriptsConfigMap
    database: overops
    existingSecret: "postgres"
  commonLabels:
    app: postgres
  primary:
    persistence:
      size: 200Gi
    resources:
      limits:
        cpu: 4
        memory: 8192Mi
      requests:
        cpu: 4
        memory: 8192Mi
clickhouse:
  fullnameOverride: "clickhouse"
  auth:
    existingSecret: "clickhouse"
    existingSecretKey: "admin-password"
  image:
    tag: 23.3.13-debian-11-r0
  podLabels: { app: clickhouse }
  resources:
    limits:
      memory: "8Gi"
    requests:
      cpu: 6
      memory: "8Gi"
  persistence:
    size: 1Ti
  defaultConfigurationOverrides: |
    <clickhouse>
      <!-- Macros -->
      <macros>
        <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
        <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
        <layer>{{ include "common.names.fullname" . }}</layer>
      </macros>
      <!-- Log Level -->
      <logger>
        <level>{{ .Values.logLevel }}</level>
      </logger>
      {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <default>
          {{- $shards := $.Values.shards | int }}
          {{- range $shard, $e := until $shards }}
          <shard>
              <internal_replication>false</internal_replication>
              {{- $replicas := $.Values.replicaCount | int }}
              {{- range $i, $_e := until $replicas }}
              <replica>
                  <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
                  <port>{{ $.Values.service.ports.tcp }}</port>
              </replica>
              {{- end }}
          </shard>
          {{- end }}
        </default>
      </remote_servers>
      {{- end }}
      {{- if .Values.keeper.enabled }}
      <!-- keeper configuration -->
      <keeper_server>
        {{/*ClickHouse keeper configuration using the helm chart */}}
        <tcp_port>{{ $.Values.containerPorts.keeper }}</tcp_port>
        {{- if .Values.tls.enabled }}
        <tcp_port_secure>{{ $.Values.containerPorts.keeperSecure }}</tcp_port_secure>
        {{- end }}
        <server_id from_env="KEEPER_SERVER_ID"></server_id>
        <log_storage_path>/bitnami/clickhouse/keeper/coordination/log</log_storage_path>
        <snapshot_storage_path>/bitnami/clickhouse/keeper/coordination/snapshots</snapshot_storage_path>

        <coordination_settings>
            <operation_timeout_ms>10000</operation_timeout_ms>
            <session_timeout_ms>30000</session_timeout_ms>
            <raft_logs_level>trace</raft_logs_level>
        </coordination_settings>

        <raft_configuration>
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <server>
          <id>{{ $node | int }}</id>
          <hostname from_env="{{ printf "KEEPER_NODE_%d" $node }}"></hostname>
          <port>{{ $.Values.service.ports.keeperInter }}</port>
        </server>
        {{- end }}
        </raft_configuration>
      </keeper_server>
      {{- end }}
      {{- if or .Values.keeper.enabled .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
      <!-- Zookeeper configuration -->
      <zookeeper>
        {{- if or .Values.keeper.enabled }}
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.service.ports.keeper }}</port>
        </node>
        {{- end }}
        {{- else if .Values.zookeeper.enabled }}
        {{/* Zookeeper configuration using the helm chart */}}
        {{- $nodes := .Values.zookeeper.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.zookeeper.service.ports.client }}</port>
        </node>
        {{- end }}
        {{- else if .Values.externalZookeeper.servers }}
        {{/* Zookeeper configuration using an external instance */}}
        {{- range $node :=.Values.externalZookeeper.servers }}
        <node>
          <host>{{ $node }}</host>
          <port>{{ $.Values.externalZookeeper.port }}</port>
        </node>
        {{- end }}
        {{- end }}
      </zookeeper>
      {{- end }}
      {{- if .Values.tls.enabled }}
      <!-- TLS configuration -->
      <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
      <openSSL>
          <server>
              {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
              {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
              <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
              <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
              <verificationMode>none</verificationMode>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
              {{- $caFileName := default "ca.crt" .Values.tls.certFilename }}
              <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
              {{- else }}
              <loadDefaultCAFile>true</loadDefaultCAFile>
              {{- end }}
          </server>
          <client>
              <loadDefaultCAFile>true</loadDefaultCAFile>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              <verificationMode>none</verificationMode>
              <invalidCertificateHandler>
                  <name>AcceptCertificateHandler</name>
              </invalidCertificateHandler>
          </client>
      </openSSL>
      {{- end }}
      {{- if .Values.metrics.enabled }}
      <!-- Prometheus metrics -->
      <prometheus>
          <endpoint>/metrics</endpoint>
          <port from_env="CLICKHOUSE_METRICS_PORT"></port>
          <metrics>true</metrics>
          <events>true</events>
          <asynchronous_metrics>true</asynchronous_metrics>
      </prometheus>
      {{- end }}
    </clickhouse>
  replicaCount: 1
  shards: 1
  zookeeper:
    enabled: false
    fullnameOverride: "clickhouse-zookeeper"
    replicaCount: 1
